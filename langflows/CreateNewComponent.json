{"description": "Adjusts React component", "icon_bg_color": null, "updated_at": "2024-10-09T03:26:26+00:00", "webhook": false, "id": "40509e8f-896a-40aa-9f51-e2ded9715233", "user_id": "a0f1898f-e833-44c7-abed-76da6f8e0530", "folder_id": "6fa8a7bd-f479-4e51-9c1d-48b32a84aa46", "name": "CreateNewComponent", "icon": null, "is_component": false, "endpoint_name": "coder-1", "data": {"nodes": [{"data": {"description": "Create a prompt template with dynamic variables.", "display_name": "Prompt", "id": "Prompt-CvKnl", "node": {"template": {"_type": "Component", "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "template": {"trace_as_input": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "template", "value": "You will output the name of a React component given a component request.\n\nIMPORTANT: Only output the name of the component with no other text.\n\nExample:\nComponent Request: I want a footer component that shows the privacy policy, toc etc....\n\nComponent Name: Footer\n\nActual:\nComponent Request: {component_request}\n\nComponent Name:", "display_name": "Template", "advanced": false, "dynamic": false, "info": "", "title_case": false, "type": "prompt", "_input_type": "PromptInput", "load_from_db": false}, "component_request": {"field_type": "str", "required": false, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "", "fileTypes": [], "file_path": "", "name": "component_request", "display_name": "component_request", "advanced": false, "input_types": ["Message", "Text"], "dynamic": false, "info": "", "load_from_db": false, "title_case": false, "type": "str"}}, "description": "Create a prompt template with dynamic variables.", "icon": "prompts", "base_classes": ["Message"], "display_name": "Prompt", "documentation": "", "custom_fields": {"template": ["component_request"]}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Message"], "selected": "Message", "name": "prompt", "display_name": "Prompt Message", "method": "build_prompt", "value": "__UNDEFINED__", "cache": true}], "field_order": ["template"], "beta": false, "edited": false, "lf_version": "1.0.18"}, "type": "Prompt"}, "dragging": false, "height": 413, "id": "Prompt-CvKnl", "position": {"x": -293.6037901324521, "y": 778.5823230912166}, "positionAbsolute": {"x": -293.6037901324521, "y": 778.5823230912166}, "selected": false, "type": "genericNode", "width": 384}, {"id": "TextInput-2VbVx", "type": "genericNode", "position": {"x": -773.1974324270875, "y": 1004.6327276823448}, "data": {"type": "TextInput", "node": {"template": {"_type": "Component", "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "from langflow.base.io.text import TextComponent\nfrom langflow.io import MultilineInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextInputComponent(TextComponent):\n    display_name = \"Text Input\"\n    description = \"Get text inputs from the Playground.\"\n    icon = \"type\"\n    name = \"TextInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as input.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n        )\n        return message\n", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "input_value": {"trace_as_input": true, "multiline": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "input_value", "value": "Write a love poem for my filipino firend", "display_name": "Text", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "Text to be passed as input.", "title_case": false, "type": "str", "_input_type": "MultilineInput"}}, "description": "Get text inputs from the Playground.", "icon": "type", "base_classes": ["Message"], "display_name": "component_request", "documentation": "", "custom_fields": {}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Message"], "selected": "Message", "name": "text", "display_name": "Text", "method": "text_response", "value": "__UNDEFINED__", "cache": true}], "field_order": ["input_value"], "beta": false, "edited": false, "lf_version": "1.0.18"}, "id": "TextInput-2VbVx", "description": "Get text inputs from the Playground.", "display_name": "component_request"}, "selected": false, "width": 384, "height": 299, "positionAbsolute": {"x": -773.1974324270875, "y": 1004.6327276823448}, "dragging": false}, {"id": "TextOutput-SDzoG", "type": "genericNode", "position": {"x": 2018.538286514424, "y": 2050.5585781553555}, "data": {"type": "TextOutput", "node": {"template": {"_type": "Component", "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "from langflow.base.io.text import TextComponent\nfrom langflow.io import MultilineInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextOutputComponent(TextComponent):\n    display_name = \"Text Output\"\n    description = \"Display a text output in the Playground.\"\n    icon = \"type\"\n    name = \"TextOutput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as output.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n        )\n        self.status = self.input_value\n        return message\n", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "input_value": {"trace_as_input": true, "multiline": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "input_value", "value": "", "display_name": "Text", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "Text to be passed as output.", "title_case": false, "type": "str", "_input_type": "MultilineInput"}}, "description": "Display a text output in the Playground.", "icon": "type", "base_classes": ["Message"], "display_name": "updated_code", "documentation": "", "custom_fields": {}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Message"], "selected": "Message", "name": "text", "display_name": "Text", "method": "text_response", "value": "__UNDEFINED__", "cache": true}], "field_order": ["input_value"], "beta": false, "edited": false, "lf_version": "1.0.18"}, "id": "TextOutput-SDzoG", "description": "Display a text output in the Playground.", "display_name": "updated_code"}, "selected": false, "width": 384, "height": 299, "positionAbsolute": {"x": 2018.538286514424, "y": 2050.5585781553555}, "dragging": false}, {"id": "Prompt-ID87b", "type": "genericNode", "position": {"x": 690.4975747187314, "y": 1488.021687798426}, "data": {"description": "Create a prompt template with dynamic variables.", "display_name": "Prompt", "id": "Prompt-ID87b", "node": {"template": {"_type": "Component", "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "template": {"trace_as_input": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "template", "value": "1. **Input Details:**\n   - Component Name: The name of the React component\n   - Component Request: The specifications of the React component\n\n2. **Output Requirements:**\n   - Component Name: The name of the React component\n   - React Component File: Provide the entire updated code of the React component, ensuring that the component matches the request.\n   - Stylesheet: Provide the entire updated stylesheet if there are any styling requirements specified in the component request.\n\n3. **Guidelines:**\n   - Write in JSX format.\n   - Ensure that the entire code and CSS are provided, not just the parts that were changed.\n   - Use the provided separators to clearly denote the JSX and CSS files in your response.\n\n4. **Example Input**\n   - **Component Name**: GreetingComponent\n   - **Component Request**: Create a React component with a heading that says \"Hello, World!\" and a blue centered text style.\n\n5. **Example Output**\n   - ** REACT COMPONENT NAME**\n```plaintext\nGreetingComponent\n```\n\n   - **REACT COMPONENT**\n    ```jsx\nimport React from 'react';\nimport './GreetingComponent.css';\n\nconst GreetingComponent = () => {{  return (<div className=\"container\"><h1>Hello, World!</h1></div>); }};\n\nexport default GreetingComponent;\n     ```\n   - **STYLESHEET**\n     ```css\n.container {{ text-align: center; color: blue; }}\n     ```\n\n6. **Input**\n   - **Component Name**: {component_name}\n   - **Component Request**: {component_request}", "display_name": "Template", "advanced": false, "dynamic": false, "info": "", "title_case": false, "type": "prompt", "_input_type": "PromptInput", "load_from_db": false}, "component_name": {"field_type": "str", "required": false, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "", "fileTypes": [], "file_path": "", "name": "component_name", "display_name": "component_name", "advanced": false, "input_types": ["Message", "Text"], "dynamic": false, "info": "", "load_from_db": false, "title_case": false, "type": "str"}, "component_request": {"field_type": "str", "required": false, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "", "fileTypes": [], "file_path": "", "name": "component_request", "display_name": "component_request", "advanced": false, "input_types": ["Message", "Text"], "dynamic": false, "info": "", "load_from_db": false, "title_case": false, "type": "str"}}, "description": "Create a prompt template with dynamic variables.", "icon": "prompts", "base_classes": ["Message"], "display_name": "Prompt", "documentation": "", "custom_fields": {"template": ["component_name", "component_request"]}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Message"], "selected": "Message", "name": "prompt", "display_name": "Prompt Message", "method": "build_prompt", "value": "__UNDEFINED__", "cache": true}], "field_order": ["template"], "beta": false, "edited": false, "lf_version": "1.0.18"}, "type": "Prompt"}, "selected": false, "width": 384, "height": 498, "positionAbsolute": {"x": 690.4975747187314, "y": 1488.021687798426}, "dragging": false}, {"id": "OllamaModel-2Sltu", "type": "genericNode", "position": {"x": 246.71840993661544, "y": 783.3751655906766}, "data": {"type": "OllamaModel", "node": {"template": {"_type": "Component", "base_url": {"trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "base_url", "value": "http://localhost:11434", "display_name": "Base URL", "advanced": false, "dynamic": false, "info": "Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.", "title_case": false, "type": "str", "_input_type": "StrInput"}, "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "from typing import Any\nfrom urllib.parse import urljoin\n\nimport httpx\nfrom langchain_community.chat_models import ChatOllama\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.io import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, StrInput\n\n\nclass ChatOllamaComponent(LCModelComponent):\n    display_name = \"Ollama\"\n    description = \"Generate text using Ollama Local LLMs.\"\n    icon = \"Ollama\"\n    name = \"OllamaModel\"\n\n    def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None):\n        if field_name == \"mirostat\":\n            if field_value == \"Disabled\":\n                build_config[\"mirostat_eta\"][\"advanced\"] = True\n                build_config[\"mirostat_tau\"][\"advanced\"] = True\n                build_config[\"mirostat_eta\"][\"value\"] = None\n                build_config[\"mirostat_tau\"][\"value\"] = None\n\n            else:\n                build_config[\"mirostat_eta\"][\"advanced\"] = False\n                build_config[\"mirostat_tau\"][\"advanced\"] = False\n\n                if field_value == \"Mirostat 2.0\":\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.2\n                    build_config[\"mirostat_tau\"][\"value\"] = 10\n                else:\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.1\n                    build_config[\"mirostat_tau\"][\"value\"] = 5\n\n        if field_name == \"model_name\":\n            base_url_dict = build_config.get(\"base_url\", {})\n            base_url_load_from_db = base_url_dict.get(\"load_from_db\", False)\n            base_url_value = base_url_dict.get(\"value\")\n            if base_url_load_from_db:\n                base_url_value = self.variables(base_url_value)\n            elif not base_url_value:\n                base_url_value = \"http://localhost:11434\"\n            build_config[\"model_name\"][\"options\"] = self.get_model(base_url_value)\n        if field_name == \"keep_alive_flag\":\n            if field_value == \"Keep\":\n                build_config[\"keep_alive\"][\"value\"] = \"-1\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            elif field_value == \"Immediately\":\n                build_config[\"keep_alive\"][\"value\"] = \"0\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            else:\n                build_config[\"keep_alive\"][\"advanced\"] = False\n\n        return build_config\n\n    def get_model(self, base_url_value: str) -> list[str]:\n        try:\n            url = urljoin(base_url_value, \"/api/tags\")\n            with httpx.Client() as client:\n                response = client.get(url)\n                response.raise_for_status()\n                data = response.json()\n\n                model_names = [model[\"name\"] for model in data.get(\"models\", [])]\n                return model_names\n        except Exception as e:\n            raise ValueError(\"Could not retrieve models. Please, make sure Ollama is running.\") from e\n\n    inputs = LCModelComponent._base_inputs + [\n        StrInput(\n            name=\"base_url\",\n            display_name=\"Base URL\",\n            info=\"Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.\",\n            value=\"http://localhost:11434\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            value=\"llama3.1\",\n            info=\"Refer to https://ollama.com/library for more models.\",\n            refresh_button=True,\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.2,\n            info=\"Controls the creativity of model responses.\",\n        ),\n        StrInput(\n            name=\"format\",\n            display_name=\"Format\",\n            info=\"Specify the format of the output (e.g., json).\",\n            advanced=True,\n        ),\n        DictInput(\n            name=\"metadata\",\n            display_name=\"Metadata\",\n            info=\"Metadata to add to the run trace.\",\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"mirostat\",\n            display_name=\"Mirostat\",\n            options=[\"Disabled\", \"Mirostat\", \"Mirostat 2.0\"],\n            info=\"Enable/disable Mirostat sampling for controlling perplexity.\",\n            value=\"Disabled\",\n            advanced=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"mirostat_eta\",\n            display_name=\"Mirostat Eta\",\n            info=\"Learning rate for Mirostat algorithm. (Default: 0.1)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"mirostat_tau\",\n            display_name=\"Mirostat Tau\",\n            info=\"Controls the balance between coherence and diversity of the output. (Default: 5.0)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_ctx\",\n            display_name=\"Context Window Size\",\n            info=\"Size of the context window for generating tokens. (Default: 2048)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_gpu\",\n            display_name=\"Number of GPUs\",\n            info=\"Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_thread\",\n            display_name=\"Number of Threads\",\n            info=\"Number of threads to use during computation. (Default: detected for optimal performance)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"repeat_last_n\",\n            display_name=\"Repeat Last N\",\n            info=\"How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"repeat_penalty\",\n            display_name=\"Repeat Penalty\",\n            info=\"Penalty for repetitions in generated text. (Default: 1.1)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"tfs_z\",\n            display_name=\"TFS Z\",\n            info=\"Tail free sampling value. (Default: 1)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"timeout\",\n            display_name=\"Timeout\",\n            info=\"Timeout for the request stream.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Limits token selection to top K. (Default: 40)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"Works together with top-k. (Default: 0.9)\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"verbose\",\n            display_name=\"Verbose\",\n            info=\"Whether to print out response text.\",\n        ),\n        StrInput(\n            name=\"tags\",\n            display_name=\"Tags\",\n            info=\"Comma-separated list of tags to add to the run trace.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"stop_tokens\",\n            display_name=\"Stop Tokens\",\n            info=\"Comma-separated list of tokens to signal the model to stop generating text.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"system\",\n            display_name=\"System\",\n            info=\"System to use for generating text.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"Template to use for generating text.\",\n            advanced=True,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # Mapping mirostat settings to their corresponding values\n        mirostat_options = {\"Mirostat\": 1, \"Mirostat 2.0\": 2}\n\n        # Default to 0 for 'Disabled'\n        mirostat_value = mirostat_options.get(self.mirostat, 0)  # type: ignore\n\n        # Set mirostat_eta and mirostat_tau to None if mirostat is disabled\n        if mirostat_value == 0:\n            mirostat_eta = None\n            mirostat_tau = None\n        else:\n            mirostat_eta = self.mirostat_eta\n            mirostat_tau = self.mirostat_tau\n\n        # Mapping system settings to their corresponding values\n        llm_params = {\n            \"base_url\": self.base_url,\n            \"model\": self.model_name,\n            \"mirostat\": mirostat_value,\n            \"format\": self.format,\n            \"metadata\": self.metadata,\n            \"tags\": self.tags.split(\",\") if self.tags else None,\n            \"mirostat_eta\": mirostat_eta,\n            \"mirostat_tau\": mirostat_tau,\n            \"num_ctx\": self.num_ctx or None,\n            \"num_gpu\": self.num_gpu or None,\n            \"num_thread\": self.num_thread or None,\n            \"repeat_last_n\": self.repeat_last_n or None,\n            \"repeat_penalty\": self.repeat_penalty or None,\n            \"temperature\": self.temperature or None,\n            \"stop\": self.stop_tokens.split(\",\") if self.stop_tokens else None,\n            \"system\": self.system,\n            \"template\": self.template,\n            \"tfs_z\": self.tfs_z or None,\n            \"timeout\": self.timeout or None,\n            \"top_k\": self.top_k or None,\n            \"top_p\": self.top_p or None,\n            \"verbose\": self.verbose,\n        }\n\n        # Remove parameters with None values\n        llm_params = {k: v for k, v in llm_params.items() if v is not None}\n\n        try:\n            output = ChatOllama(**llm_params)  # type: ignore\n        except Exception as e:\n            raise ValueError(\"Could not initialize Ollama LLM.\") from e\n\n        return output  # type: ignore\n", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "format": {"trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "format", "value": "", "display_name": "Format", "advanced": true, "dynamic": false, "info": "Specify the format of the output (e.g., json).", "title_case": false, "type": "str", "_input_type": "StrInput"}, "input_value": {"trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "input_value", "value": "", "display_name": "Input", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "", "title_case": false, "type": "str", "_input_type": "MessageInput"}, "metadata": {"trace_as_input": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "metadata", "value": {}, "display_name": "Metadata", "advanced": true, "dynamic": false, "info": "Metadata to add to the run trace.", "title_case": false, "type": "dict", "_input_type": "DictInput"}, "mirostat": {"trace_as_metadata": true, "options": ["Disabled", "Mirostat", "Mirostat 2.0"], "combobox": false, "required": false, "placeholder": "", "show": true, "name": "mirostat", "value": "Disabled", "display_name": "Mirostat", "advanced": true, "dynamic": false, "info": "Enable/disable Mirostat sampling for controlling perplexity.", "real_time_refresh": true, "title_case": false, "type": "str", "_input_type": "DropdownInput"}, "mirostat_eta": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "mirostat_eta", "value": "", "display_name": "Mirostat Eta", "advanced": true, "dynamic": false, "info": "Learning rate for Mirostat algorithm. (Default: 0.1)", "title_case": false, "type": "float", "_input_type": "FloatInput"}, "mirostat_tau": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "mirostat_tau", "value": "", "display_name": "Mirostat Tau", "advanced": true, "dynamic": false, "info": "Controls the balance between coherence and diversity of the output. (Default: 5.0)", "title_case": false, "type": "float", "_input_type": "FloatInput"}, "model_name": {"trace_as_metadata": true, "options": ["llama3.1:latest", "mistral:latest"], "combobox": false, "required": false, "placeholder": "", "show": true, "name": "model_name", "value": "llama3.1:latest", "display_name": "Model Name", "advanced": false, "dynamic": false, "info": "Refer to https://ollama.com/library for more models.", "refresh_button": true, "title_case": false, "type": "str", "_input_type": "DropdownInput"}, "num_ctx": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "num_ctx", "value": "", "display_name": "Context Window Size", "advanced": true, "dynamic": false, "info": "Size of the context window for generating tokens. (Default: 2048)", "title_case": false, "type": "int", "_input_type": "IntInput"}, "num_gpu": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "num_gpu", "value": "", "display_name": "Number of GPUs", "advanced": true, "dynamic": false, "info": "Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)", "title_case": false, "type": "int", "_input_type": "IntInput"}, "num_thread": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "num_thread", "value": "", "display_name": "Number of Threads", "advanced": true, "dynamic": false, "info": "Number of threads to use during computation. (Default: detected for optimal performance)", "title_case": false, "type": "int", "_input_type": "IntInput"}, "repeat_last_n": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "repeat_last_n", "value": "", "display_name": "Repeat Last N", "advanced": true, "dynamic": false, "info": "How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)", "title_case": false, "type": "int", "_input_type": "IntInput"}, "repeat_penalty": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "repeat_penalty", "value": "", "display_name": "Repeat Penalty", "advanced": true, "dynamic": false, "info": "Penalty for repetitions in generated text. (Default: 1.1)", "title_case": false, "type": "float", "_input_type": "FloatInput"}, "stop_tokens": {"trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "stop_tokens", "value": "", "display_name": "Stop Tokens", "advanced": true, "dynamic": false, "info": "Comma-separated list of tokens to signal the model to stop generating text.", "title_case": false, "type": "str", "_input_type": "StrInput"}, "stream": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "stream", "value": false, "display_name": "Stream", "advanced": true, "dynamic": false, "info": "Stream the response from the model. Streaming works only in Chat.", "title_case": false, "type": "bool", "_input_type": "BoolInput"}, "system": {"trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "system", "value": "", "display_name": "System", "advanced": true, "dynamic": false, "info": "System to use for generating text.", "title_case": false, "type": "str", "_input_type": "StrInput"}, "system_message": {"trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "system_message", "value": "", "display_name": "System Message", "advanced": true, "input_types": ["Message"], "dynamic": false, "info": "System message to pass to the model.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "tags": {"trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "tags", "value": "", "display_name": "Tags", "advanced": true, "dynamic": false, "info": "Comma-separated list of tags to add to the run trace.", "title_case": false, "type": "str", "_input_type": "StrInput"}, "temperature": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "temperature", "value": 0.2, "display_name": "Temperature", "advanced": false, "dynamic": false, "info": "Controls the creativity of model responses.", "title_case": false, "type": "float", "_input_type": "FloatInput"}, "template": {"trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "template", "value": "", "display_name": "Template", "advanced": true, "dynamic": false, "info": "Template to use for generating text.", "title_case": false, "type": "str", "_input_type": "StrInput"}, "tfs_z": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "tfs_z", "value": "", "display_name": "TFS Z", "advanced": true, "dynamic": false, "info": "Tail free sampling value. (Default: 1)", "title_case": false, "type": "float", "_input_type": "FloatInput"}, "timeout": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "timeout", "value": "", "display_name": "Timeout", "advanced": true, "dynamic": false, "info": "Timeout for the request stream.", "title_case": false, "type": "int", "_input_type": "IntInput"}, "top_k": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "top_k", "value": "", "display_name": "Top K", "advanced": true, "dynamic": false, "info": "Limits token selection to top K. (Default: 40)", "title_case": false, "type": "int", "_input_type": "IntInput"}, "top_p": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "top_p", "value": "", "display_name": "Top P", "advanced": true, "dynamic": false, "info": "Works together with top-k. (Default: 0.9)", "title_case": false, "type": "float", "_input_type": "FloatInput"}, "verbose": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "verbose", "value": true, "display_name": "Verbose", "advanced": false, "dynamic": false, "info": "Whether to print out response text.", "title_case": false, "type": "bool", "_input_type": "BoolInput"}}, "description": "Generate text using Ollama Local LLMs.", "icon": "Ollama", "base_classes": ["LanguageModel", "Message"], "display_name": "Ollama", "documentation": "", "custom_fields": {}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Message"], "selected": "Message", "name": "text_output", "display_name": "Text", "method": "text_response", "value": "__UNDEFINED__", "cache": true}, {"types": ["LanguageModel"], "selected": "LanguageModel", "name": "model_output", "display_name": "Language Model", "method": "build_model", "value": "__UNDEFINED__", "cache": true}], "field_order": ["input_value", "system_message", "stream", "base_url", "model_name", "temperature", "format", "metadata", "mirostat", "mirostat_eta", "mirostat_tau", "num_ctx", "num_gpu", "num_thread", "repeat_last_n", "repeat_penalty", "tfs_z", "timeout", "top_k", "top_p", "verbose", "tags", "stop_tokens", "system", "template"], "beta": false, "edited": false, "lf_version": "1.0.18"}, "id": "OllamaModel-2Sltu"}, "selected": false, "width": 384, "height": 679, "positionAbsolute": {"x": 246.71840993661544, "y": 783.3751655906766}, "dragging": false}, {"id": "OllamaModel-WTYh0", "type": "genericNode", "position": {"x": 1509.6371775167859, "y": 1434.041299438967}, "data": {"type": "OllamaModel", "node": {"template": {"_type": "Component", "base_url": {"trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "base_url", "value": "http://localhost:11434", "display_name": "Base URL", "advanced": false, "dynamic": false, "info": "Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.", "title_case": false, "type": "str", "_input_type": "StrInput"}, "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "from typing import Any\nfrom urllib.parse import urljoin\n\nimport httpx\nfrom langchain_community.chat_models import ChatOllama\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.io import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, StrInput\n\n\nclass ChatOllamaComponent(LCModelComponent):\n    display_name = \"Ollama\"\n    description = \"Generate text using Ollama Local LLMs.\"\n    icon = \"Ollama\"\n    name = \"OllamaModel\"\n\n    def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None):\n        if field_name == \"mirostat\":\n            if field_value == \"Disabled\":\n                build_config[\"mirostat_eta\"][\"advanced\"] = True\n                build_config[\"mirostat_tau\"][\"advanced\"] = True\n                build_config[\"mirostat_eta\"][\"value\"] = None\n                build_config[\"mirostat_tau\"][\"value\"] = None\n\n            else:\n                build_config[\"mirostat_eta\"][\"advanced\"] = False\n                build_config[\"mirostat_tau\"][\"advanced\"] = False\n\n                if field_value == \"Mirostat 2.0\":\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.2\n                    build_config[\"mirostat_tau\"][\"value\"] = 10\n                else:\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.1\n                    build_config[\"mirostat_tau\"][\"value\"] = 5\n\n        if field_name == \"model_name\":\n            base_url_dict = build_config.get(\"base_url\", {})\n            base_url_load_from_db = base_url_dict.get(\"load_from_db\", False)\n            base_url_value = base_url_dict.get(\"value\")\n            if base_url_load_from_db:\n                base_url_value = self.variables(base_url_value)\n            elif not base_url_value:\n                base_url_value = \"http://localhost:11434\"\n            build_config[\"model_name\"][\"options\"] = self.get_model(base_url_value)\n        if field_name == \"keep_alive_flag\":\n            if field_value == \"Keep\":\n                build_config[\"keep_alive\"][\"value\"] = \"-1\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            elif field_value == \"Immediately\":\n                build_config[\"keep_alive\"][\"value\"] = \"0\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            else:\n                build_config[\"keep_alive\"][\"advanced\"] = False\n\n        return build_config\n\n    def get_model(self, base_url_value: str) -> list[str]:\n        try:\n            url = urljoin(base_url_value, \"/api/tags\")\n            with httpx.Client() as client:\n                response = client.get(url)\n                response.raise_for_status()\n                data = response.json()\n\n                model_names = [model[\"name\"] for model in data.get(\"models\", [])]\n                return model_names\n        except Exception as e:\n            raise ValueError(\"Could not retrieve models. Please, make sure Ollama is running.\") from e\n\n    inputs = LCModelComponent._base_inputs + [\n        StrInput(\n            name=\"base_url\",\n            display_name=\"Base URL\",\n            info=\"Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.\",\n            value=\"http://localhost:11434\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            value=\"llama3.1\",\n            info=\"Refer to https://ollama.com/library for more models.\",\n            refresh_button=True,\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.2,\n            info=\"Controls the creativity of model responses.\",\n        ),\n        StrInput(\n            name=\"format\",\n            display_name=\"Format\",\n            info=\"Specify the format of the output (e.g., json).\",\n            advanced=True,\n        ),\n        DictInput(\n            name=\"metadata\",\n            display_name=\"Metadata\",\n            info=\"Metadata to add to the run trace.\",\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"mirostat\",\n            display_name=\"Mirostat\",\n            options=[\"Disabled\", \"Mirostat\", \"Mirostat 2.0\"],\n            info=\"Enable/disable Mirostat sampling for controlling perplexity.\",\n            value=\"Disabled\",\n            advanced=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"mirostat_eta\",\n            display_name=\"Mirostat Eta\",\n            info=\"Learning rate for Mirostat algorithm. (Default: 0.1)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"mirostat_tau\",\n            display_name=\"Mirostat Tau\",\n            info=\"Controls the balance between coherence and diversity of the output. (Default: 5.0)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_ctx\",\n            display_name=\"Context Window Size\",\n            info=\"Size of the context window for generating tokens. (Default: 2048)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_gpu\",\n            display_name=\"Number of GPUs\",\n            info=\"Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_thread\",\n            display_name=\"Number of Threads\",\n            info=\"Number of threads to use during computation. (Default: detected for optimal performance)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"repeat_last_n\",\n            display_name=\"Repeat Last N\",\n            info=\"How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"repeat_penalty\",\n            display_name=\"Repeat Penalty\",\n            info=\"Penalty for repetitions in generated text. (Default: 1.1)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"tfs_z\",\n            display_name=\"TFS Z\",\n            info=\"Tail free sampling value. (Default: 1)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"timeout\",\n            display_name=\"Timeout\",\n            info=\"Timeout for the request stream.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Limits token selection to top K. (Default: 40)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"Works together with top-k. (Default: 0.9)\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"verbose\",\n            display_name=\"Verbose\",\n            info=\"Whether to print out response text.\",\n        ),\n        StrInput(\n            name=\"tags\",\n            display_name=\"Tags\",\n            info=\"Comma-separated list of tags to add to the run trace.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"stop_tokens\",\n            display_name=\"Stop Tokens\",\n            info=\"Comma-separated list of tokens to signal the model to stop generating text.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"system\",\n            display_name=\"System\",\n            info=\"System to use for generating text.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"Template to use for generating text.\",\n            advanced=True,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # Mapping mirostat settings to their corresponding values\n        mirostat_options = {\"Mirostat\": 1, \"Mirostat 2.0\": 2}\n\n        # Default to 0 for 'Disabled'\n        mirostat_value = mirostat_options.get(self.mirostat, 0)  # type: ignore\n\n        # Set mirostat_eta and mirostat_tau to None if mirostat is disabled\n        if mirostat_value == 0:\n            mirostat_eta = None\n            mirostat_tau = None\n        else:\n            mirostat_eta = self.mirostat_eta\n            mirostat_tau = self.mirostat_tau\n\n        # Mapping system settings to their corresponding values\n        llm_params = {\n            \"base_url\": self.base_url,\n            \"model\": self.model_name,\n            \"mirostat\": mirostat_value,\n            \"format\": self.format,\n            \"metadata\": self.metadata,\n            \"tags\": self.tags.split(\",\") if self.tags else None,\n            \"mirostat_eta\": mirostat_eta,\n            \"mirostat_tau\": mirostat_tau,\n            \"num_ctx\": self.num_ctx or None,\n            \"num_gpu\": self.num_gpu or None,\n            \"num_thread\": self.num_thread or None,\n            \"repeat_last_n\": self.repeat_last_n or None,\n            \"repeat_penalty\": self.repeat_penalty or None,\n            \"temperature\": self.temperature or None,\n            \"stop\": self.stop_tokens.split(\",\") if self.stop_tokens else None,\n            \"system\": self.system,\n            \"template\": self.template,\n            \"tfs_z\": self.tfs_z or None,\n            \"timeout\": self.timeout or None,\n            \"top_k\": self.top_k or None,\n            \"top_p\": self.top_p or None,\n            \"verbose\": self.verbose,\n        }\n\n        # Remove parameters with None values\n        llm_params = {k: v for k, v in llm_params.items() if v is not None}\n\n        try:\n            output = ChatOllama(**llm_params)  # type: ignore\n        except Exception as e:\n            raise ValueError(\"Could not initialize Ollama LLM.\") from e\n\n        return output  # type: ignore\n", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "format": {"trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "format", "value": "", "display_name": "Format", "advanced": true, "dynamic": false, "info": "Specify the format of the output (e.g., json).", "title_case": false, "type": "str", "_input_type": "StrInput"}, "input_value": {"trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "input_value", "value": "", "display_name": "Input", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "", "title_case": false, "type": "str", "_input_type": "MessageInput"}, "metadata": {"trace_as_input": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "metadata", "value": {}, "display_name": "Metadata", "advanced": true, "dynamic": false, "info": "Metadata to add to the run trace.", "title_case": false, "type": "dict", "_input_type": "DictInput"}, "mirostat": {"trace_as_metadata": true, "options": ["Disabled", "Mirostat", "Mirostat 2.0"], "combobox": false, "required": false, "placeholder": "", "show": true, "name": "mirostat", "value": "Disabled", "display_name": "Mirostat", "advanced": true, "dynamic": false, "info": "Enable/disable Mirostat sampling for controlling perplexity.", "real_time_refresh": true, "title_case": false, "type": "str", "_input_type": "DropdownInput"}, "mirostat_eta": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "mirostat_eta", "value": "", "display_name": "Mirostat Eta", "advanced": true, "dynamic": false, "info": "Learning rate for Mirostat algorithm. (Default: 0.1)", "title_case": false, "type": "float", "_input_type": "FloatInput"}, "mirostat_tau": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "mirostat_tau", "value": "", "display_name": "Mirostat Tau", "advanced": true, "dynamic": false, "info": "Controls the balance between coherence and diversity of the output. (Default: 5.0)", "title_case": false, "type": "float", "_input_type": "FloatInput"}, "model_name": {"trace_as_metadata": true, "options": ["llama3.1:latest", "mistral:latest"], "combobox": false, "required": false, "placeholder": "", "show": true, "name": "model_name", "value": "llama3.1", "display_name": "Model Name", "advanced": false, "dynamic": false, "info": "Refer to https://ollama.com/library for more models.", "refresh_button": true, "title_case": false, "type": "str", "_input_type": "DropdownInput"}, "num_ctx": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "num_ctx", "value": "", "display_name": "Context Window Size", "advanced": true, "dynamic": false, "info": "Size of the context window for generating tokens. (Default: 2048)", "title_case": false, "type": "int", "_input_type": "IntInput"}, "num_gpu": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "num_gpu", "value": "", "display_name": "Number of GPUs", "advanced": true, "dynamic": false, "info": "Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)", "title_case": false, "type": "int", "_input_type": "IntInput"}, "num_thread": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "num_thread", "value": "", "display_name": "Number of Threads", "advanced": true, "dynamic": false, "info": "Number of threads to use during computation. (Default: detected for optimal performance)", "title_case": false, "type": "int", "_input_type": "IntInput"}, "repeat_last_n": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "repeat_last_n", "value": "", "display_name": "Repeat Last N", "advanced": true, "dynamic": false, "info": "How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)", "title_case": false, "type": "int", "_input_type": "IntInput"}, "repeat_penalty": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "repeat_penalty", "value": "", "display_name": "Repeat Penalty", "advanced": true, "dynamic": false, "info": "Penalty for repetitions in generated text. (Default: 1.1)", "title_case": false, "type": "float", "_input_type": "FloatInput"}, "stop_tokens": {"trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "stop_tokens", "value": "", "display_name": "Stop Tokens", "advanced": true, "dynamic": false, "info": "Comma-separated list of tokens to signal the model to stop generating text.", "title_case": false, "type": "str", "_input_type": "StrInput"}, "stream": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "stream", "value": false, "display_name": "Stream", "advanced": true, "dynamic": false, "info": "Stream the response from the model. Streaming works only in Chat.", "title_case": false, "type": "bool", "_input_type": "BoolInput"}, "system": {"trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "system", "value": "", "display_name": "System", "advanced": true, "dynamic": false, "info": "System to use for generating text.", "title_case": false, "type": "str", "_input_type": "StrInput"}, "system_message": {"trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "system_message", "value": "", "display_name": "System Message", "advanced": true, "input_types": ["Message"], "dynamic": false, "info": "System message to pass to the model.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "tags": {"trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "tags", "value": "", "display_name": "Tags", "advanced": true, "dynamic": false, "info": "Comma-separated list of tags to add to the run trace.", "title_case": false, "type": "str", "_input_type": "StrInput"}, "temperature": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "temperature", "value": 0.2, "display_name": "Temperature", "advanced": false, "dynamic": false, "info": "Controls the creativity of model responses.", "title_case": false, "type": "float", "_input_type": "FloatInput"}, "template": {"trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "template", "value": "", "display_name": "Template", "advanced": true, "dynamic": false, "info": "Template to use for generating text.", "title_case": false, "type": "str", "_input_type": "StrInput"}, "tfs_z": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "tfs_z", "value": "", "display_name": "TFS Z", "advanced": true, "dynamic": false, "info": "Tail free sampling value. (Default: 1)", "title_case": false, "type": "float", "_input_type": "FloatInput"}, "timeout": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "timeout", "value": "", "display_name": "Timeout", "advanced": true, "dynamic": false, "info": "Timeout for the request stream.", "title_case": false, "type": "int", "_input_type": "IntInput"}, "top_k": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "top_k", "value": "", "display_name": "Top K", "advanced": true, "dynamic": false, "info": "Limits token selection to top K. (Default: 40)", "title_case": false, "type": "int", "_input_type": "IntInput"}, "top_p": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "top_p", "value": "", "display_name": "Top P", "advanced": true, "dynamic": false, "info": "Works together with top-k. (Default: 0.9)", "title_case": false, "type": "float", "_input_type": "FloatInput"}, "verbose": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "verbose", "value": true, "display_name": "Verbose", "advanced": false, "dynamic": false, "info": "Whether to print out response text.", "title_case": false, "type": "bool", "_input_type": "BoolInput"}}, "description": "Generate text using Ollama Local LLMs.", "icon": "Ollama", "base_classes": ["LanguageModel", "Message"], "display_name": "Ollama", "documentation": "", "custom_fields": {}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Message"], "selected": "Message", "name": "text_output", "display_name": "Text", "method": "text_response", "value": "__UNDEFINED__", "cache": true}, {"types": ["LanguageModel"], "selected": "LanguageModel", "name": "model_output", "display_name": "Language Model", "method": "build_model", "value": "__UNDEFINED__", "cache": true}], "field_order": ["input_value", "system_message", "stream", "base_url", "model_name", "temperature", "format", "metadata", "mirostat", "mirostat_eta", "mirostat_tau", "num_ctx", "num_gpu", "num_thread", "repeat_last_n", "repeat_penalty", "tfs_z", "timeout", "top_k", "top_p", "verbose", "tags", "stop_tokens", "system", "template"], "beta": false, "edited": false, "lf_version": "1.0.18"}, "id": "OllamaModel-WTYh0"}, "selected": true, "width": 384, "height": 679, "positionAbsolute": {"x": 1509.6371775167859, "y": 1434.041299438967}, "dragging": false}], "edges": [{"source": "TextInput-2VbVx", "sourceHandle": "{\u0153dataType\u0153:\u0153TextInput\u0153,\u0153id\u0153:\u0153TextInput-2VbVx\u0153,\u0153name\u0153:\u0153text\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}", "target": "Prompt-CvKnl", "targetHandle": "{\u0153fieldName\u0153:\u0153component_request\u0153,\u0153id\u0153:\u0153Prompt-CvKnl\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153,\u0153Text\u0153],\u0153type\u0153:\u0153str\u0153}", "data": {"targetHandle": {"fieldName": "component_request", "id": "Prompt-CvKnl", "inputTypes": ["Message", "Text"], "type": "str"}, "sourceHandle": {"dataType": "TextInput", "id": "TextInput-2VbVx", "name": "text", "output_types": ["Message"]}}, "id": "reactflow__edge-TextInput-2VbVx{\u0153dataType\u0153:\u0153TextInput\u0153,\u0153id\u0153:\u0153TextInput-2VbVx\u0153,\u0153name\u0153:\u0153text\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}-Prompt-CvKnl{\u0153fieldName\u0153:\u0153component_request\u0153,\u0153id\u0153:\u0153Prompt-CvKnl\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153,\u0153Text\u0153],\u0153type\u0153:\u0153str\u0153}", "className": ""}, {"source": "TextInput-2VbVx", "sourceHandle": "{\u0153dataType\u0153:\u0153TextInput\u0153,\u0153id\u0153:\u0153TextInput-2VbVx\u0153,\u0153name\u0153:\u0153text\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}", "target": "Prompt-ID87b", "targetHandle": "{\u0153fieldName\u0153:\u0153component_request\u0153,\u0153id\u0153:\u0153Prompt-ID87b\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153,\u0153Text\u0153],\u0153type\u0153:\u0153str\u0153}", "data": {"targetHandle": {"fieldName": "component_request", "id": "Prompt-ID87b", "inputTypes": ["Message", "Text"], "type": "str"}, "sourceHandle": {"dataType": "TextInput", "id": "TextInput-2VbVx", "name": "text", "output_types": ["Message"]}}, "id": "reactflow__edge-TextInput-2VbVx{\u0153dataType\u0153:\u0153TextInput\u0153,\u0153id\u0153:\u0153TextInput-2VbVx\u0153,\u0153name\u0153:\u0153text\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}-Prompt-ID87b{\u0153fieldName\u0153:\u0153component_request\u0153,\u0153id\u0153:\u0153Prompt-ID87b\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153,\u0153Text\u0153],\u0153type\u0153:\u0153str\u0153}", "className": ""}, {"source": "Prompt-CvKnl", "sourceHandle": "{\u0153dataType\u0153:\u0153Prompt\u0153,\u0153id\u0153:\u0153Prompt-CvKnl\u0153,\u0153name\u0153:\u0153prompt\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}", "target": "OllamaModel-2Sltu", "targetHandle": "{\u0153fieldName\u0153:\u0153input_value\u0153,\u0153id\u0153:\u0153OllamaModel-2Sltu\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "data": {"targetHandle": {"fieldName": "input_value", "id": "OllamaModel-2Sltu", "inputTypes": ["Message"], "type": "str"}, "sourceHandle": {"dataType": "Prompt", "id": "Prompt-CvKnl", "name": "prompt", "output_types": ["Message"]}}, "id": "reactflow__edge-Prompt-CvKnl{\u0153dataType\u0153:\u0153Prompt\u0153,\u0153id\u0153:\u0153Prompt-CvKnl\u0153,\u0153name\u0153:\u0153prompt\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}-OllamaModel-2Sltu{\u0153fieldName\u0153:\u0153input_value\u0153,\u0153id\u0153:\u0153OllamaModel-2Sltu\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}"}, {"source": "OllamaModel-2Sltu", "sourceHandle": "{\u0153dataType\u0153:\u0153OllamaModel\u0153,\u0153id\u0153:\u0153OllamaModel-2Sltu\u0153,\u0153name\u0153:\u0153text_output\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}", "target": "Prompt-ID87b", "targetHandle": "{\u0153fieldName\u0153:\u0153component_name\u0153,\u0153id\u0153:\u0153Prompt-ID87b\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153,\u0153Text\u0153],\u0153type\u0153:\u0153str\u0153}", "data": {"targetHandle": {"fieldName": "component_name", "id": "Prompt-ID87b", "inputTypes": ["Message", "Text"], "type": "str"}, "sourceHandle": {"dataType": "OllamaModel", "id": "OllamaModel-2Sltu", "name": "text_output", "output_types": ["Message"]}}, "id": "reactflow__edge-OllamaModel-2Sltu{\u0153dataType\u0153:\u0153OllamaModel\u0153,\u0153id\u0153:\u0153OllamaModel-2Sltu\u0153,\u0153name\u0153:\u0153text_output\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}-Prompt-ID87b{\u0153fieldName\u0153:\u0153component_name\u0153,\u0153id\u0153:\u0153Prompt-ID87b\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153,\u0153Text\u0153],\u0153type\u0153:\u0153str\u0153}"}, {"source": "Prompt-ID87b", "sourceHandle": "{\u0153dataType\u0153:\u0153Prompt\u0153,\u0153id\u0153:\u0153Prompt-ID87b\u0153,\u0153name\u0153:\u0153prompt\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}", "target": "OllamaModel-WTYh0", "targetHandle": "{\u0153fieldName\u0153:\u0153input_value\u0153,\u0153id\u0153:\u0153OllamaModel-WTYh0\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "data": {"targetHandle": {"fieldName": "input_value", "id": "OllamaModel-WTYh0", "inputTypes": ["Message"], "type": "str"}, "sourceHandle": {"dataType": "Prompt", "id": "Prompt-ID87b", "name": "prompt", "output_types": ["Message"]}}, "id": "reactflow__edge-Prompt-ID87b{\u0153dataType\u0153:\u0153Prompt\u0153,\u0153id\u0153:\u0153Prompt-ID87b\u0153,\u0153name\u0153:\u0153prompt\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}-OllamaModel-WTYh0{\u0153fieldName\u0153:\u0153input_value\u0153,\u0153id\u0153:\u0153OllamaModel-WTYh0\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}"}, {"source": "OllamaModel-WTYh0", "sourceHandle": "{\u0153dataType\u0153:\u0153OllamaModel\u0153,\u0153id\u0153:\u0153OllamaModel-WTYh0\u0153,\u0153name\u0153:\u0153text_output\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}", "target": "TextOutput-SDzoG", "targetHandle": "{\u0153fieldName\u0153:\u0153input_value\u0153,\u0153id\u0153:\u0153TextOutput-SDzoG\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "data": {"targetHandle": {"fieldName": "input_value", "id": "TextOutput-SDzoG", "inputTypes": ["Message"], "type": "str"}, "sourceHandle": {"dataType": "OllamaModel", "id": "OllamaModel-WTYh0", "name": "text_output", "output_types": ["Message"]}}, "id": "reactflow__edge-OllamaModel-WTYh0{\u0153dataType\u0153:\u0153OllamaModel\u0153,\u0153id\u0153:\u0153OllamaModel-WTYh0\u0153,\u0153name\u0153:\u0153text_output\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}-TextOutput-SDzoG{\u0153fieldName\u0153:\u0153input_value\u0153,\u0153id\u0153:\u0153TextOutput-SDzoG\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}"}], "viewport": {"x": 104.10217946798412, "y": -468.78151482923954, "zoom": 0.47410517365920907}}}